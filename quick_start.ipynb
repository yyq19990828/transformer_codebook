{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  情感分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用`pipeline`加载预测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://hf-mirror.com/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行单个句子推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"We are very happy to show you the 🤗 Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量推理(需要一个列表输入)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  在 pipeline 中使用另一个模型和分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7272651791572571}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del classifier, model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  自动推导类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载一个分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast\n",
      "transformers.models.bert.tokenization_bert_fast\n",
      "Help on method __call__ in module transformers.tokenization_utils_base:\n",
      "\n",
      "__call__(text: Union[str, List[str], List[List[str]]] = None, text_pair: Union[str, List[str], List[List[str]], NoneType] = None, text_target: Union[str, List[str], List[List[str]]] = None, text_pair_target: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[str] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.bert.tokenization_bert_fast.BertTokenizerFast instance\n",
      "    Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      "    sequences.\n",
      "    \n",
      "    Args:\n",
      "        text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "        text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "        text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "        text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    \n",
      "        add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      "            `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      "            automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n",
      "            automatically.\n",
      "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      "            Activates and controls padding. Accepts the following values:\n",
      "    \n",
      "            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      "              sequence if provided).\n",
      "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      "              acceptable input length for the model if that argument is not provided.\n",
      "            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      "              lengths).\n",
      "        truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      "            Activates and controls truncation. Accepts the following values:\n",
      "    \n",
      "            - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      "              to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      "              truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      "              sequences (or a batch of pairs) is provided.\n",
      "            - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "              truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "            - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "              truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "            - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      "              greater than the model maximum admissible input size).\n",
      "        max_length (`int`, *optional*):\n",
      "            Controls the maximum length to use by one of the truncation/padding parameters.\n",
      "    \n",
      "            If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      "            is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      "            length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "        stride (`int`, *optional*, defaults to 0):\n",
      "            If set to a number along with `max_length`, the overflowing tokens returned when\n",
      "            `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      "            returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      "            argument defines the number of overlapping tokens.\n",
      "        is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      "            tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      "            which it will tokenize. This is useful for NER or token classification.\n",
      "        pad_to_multiple_of (`int`, *optional*):\n",
      "            If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      "            `>= 7.5` (Volta).\n",
      "        padding_side (`str`, *optional*):\n",
      "            The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      "            Default value is picked from the class attribute of the same name.\n",
      "        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "            If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "    \n",
      "            - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      "            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "            - `'np'`: Return Numpy `np.ndarray` objects.\n",
      "    \n",
      "        return_token_type_ids (`bool`, *optional*):\n",
      "            Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      "            the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "    \n",
      "            [What are token type IDs?](../glossary#token-type-ids)\n",
      "        return_attention_mask (`bool`, *optional*):\n",
      "            Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "            to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "    \n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "        return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      "            of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      "            of returning overflowing tokens.\n",
      "        return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return special tokens mask information.\n",
      "        return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return `(char_start, char_end)` for each token.\n",
      "    \n",
      "            This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      "            Python's tokenizer, this method will raise `NotImplementedError`.\n",
      "        return_length  (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to return the lengths of the encoded inputs.\n",
      "        verbose (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to print more information and warnings.\n",
      "        **kwargs: passed to the `self.tokenize()` method\n",
      "    \n",
      "    Return:\n",
      "        [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      "    \n",
      "        - **input_ids** -- List of token ids to be fed to a model.\n",
      "    \n",
      "          [What are input IDs?](../glossary#input-ids)\n",
      "    \n",
      "        - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      "          if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      "    \n",
      "          [What are token type IDs?](../glossary#token-type-ids)\n",
      "    \n",
      "        - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      "          `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      "    \n",
      "          [What are attention masks?](../glossary#attention-mask)\n",
      "    \n",
      "        - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      "          `return_overflowing_tokens=True`).\n",
      "        - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      "          `return_overflowing_tokens=True`).\n",
      "        - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      "          regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      "        - **length** -- The length of the inputs (when `return_length=True`)\n",
      "\n",
      "['_add_tokens', '_batch_encode_plus', '_call_one', '_convert_encoding', '_convert_id_to_token', '_convert_token_to_id_with_added_voc', '_create_repo', '_decode', '_encode_plus', '_eventual_warn_about_too_long_sequence', '_eventually_correct_t5_max_length', '_from_pretrained', '_get_files_timestamps', '_get_padding_truncation_strategies', '_pad', '_save_pretrained', '_set_model_specific_special_tokens', '_set_processor_class', '_switch_to_input_mode', '_switch_to_target_mode', '_upload_modified_files', 'add_special_tokens', 'add_tokens', 'apply_chat_template', 'as_target_tokenizer', 'batch_decode', 'batch_encode_plus', 'build_inputs_with_special_tokens', 'clean_up_tokenization', 'convert_added_tokens', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string', 'create_token_type_ids_from_sequences', 'decode', 'encode', 'encode_plus', 'from_pretrained', 'get_added_vocab', 'get_chat_template', 'get_special_tokens_mask', 'get_vocab', 'num_special_tokens_to_add', 'pad', 'prepare_for_model', 'prepare_seq2seq_batch', 'push_to_hub', 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'set_truncation_and_padding', 'slow_tokenizer_class', 'tokenize', 'train_new_from_iterator', 'truncate_sequences']\n"
     ]
    }
   ],
   "source": [
    "tokenizer.__getattr__\n",
    "print(tokenizer.__class__.__name__)\n",
    "print(tokenizer.__module__)\n",
    "\n",
    "help(tokenizer.__call__)\n",
    "# 假设您的 tokenizer 对象名为 tokenizer\n",
    "methods = [attr for attr in dir(tokenizer) if callable(getattr(tokenizer, attr)) and not attr.startswith('__')]\n",
    "print(methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本传入分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单个句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] we are very happy to show you the [UNK] transformers library. [SEP]'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量传入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103,   100,\n",
      "         58263, 13299,   119,   102],\n",
      "        [  101, 11312, 18763, 10855, 11530,   112,   162, 39487, 10197,   119,\n",
      "           102,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(pt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] we are very happy to show you the [UNK] transformers library. [SEP]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(pt_batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method _wrapped_call_impl in module torch.nn.modules.module:\n",
      "\n",
      "_wrapped_call_impl(*args, **kwargs) method of transformers.models.bert.modeling_bert.BertForSequenceClassification instance\n",
      "\n",
      "Help on method forward in module transformers.models.bert.modeling_bert:\n",
      "\n",
      "forward(input_ids: Optional[torch.Tensor] = None, attention_mask: Optional[torch.Tensor] = None, token_type_ids: Optional[torch.Tensor] = None, position_ids: Optional[torch.Tensor] = None, head_mask: Optional[torch.Tensor] = None, inputs_embeds: Optional[torch.Tensor] = None, labels: Optional[torch.Tensor] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None) -> Union[Tuple[torch.Tensor], transformers.modeling_outputs.SequenceClassifierOutput] method of transformers.models.bert.modeling_bert.BertForSequenceClassification instance\n",
      "    The [`BertForSequenceClassification`] forward method, overrides the `__call__` special method.\n",
      "    \n",
      "    <Tip>\n",
      "    \n",
      "    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "    the latter silently ignores them.\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Args:\n",
      "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "            Indices of input sequence tokens in the vocabulary.\n",
      "    \n",
      "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "    \n",
      "            [What are input IDs?](../glossary#input-ids)\n",
      "        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`or `(batch_size, sequence_length, target_length)`, *optional*):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "    \n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "    \n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n",
      "            1]`:\n",
      "    \n",
      "            - 0 corresponds to a *sentence A* token,\n",
      "            - 1 corresponds to a *sentence B* token.\n",
      "    \n",
      "            [What are token type IDs?](../glossary#token-type-ids)\n",
      "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
      "            config.max_position_embeddings - 1]`.\n",
      "    \n",
      "            [What are position IDs?](../glossary#position-ids)\n",
      "        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "    \n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "    \n",
      "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
      "            model's internal embedding lookup matrix.\n",
      "        output_attentions (`bool`, *optional*):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "            tensors for more detail.\n",
      "        output_hidden_states (`bool`, *optional*):\n",
      "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "            more detail.\n",
      "        return_dict (`bool`, *optional*):\n",
      "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "    \n",
      "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
      "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
      "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
      "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
      "        \n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.SequenceClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.SequenceClassifierOutput`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`BertConfig`]) and inputs.\n",
      "    \n",
      "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
      "        - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
      "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "    \n",
      "          Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "    \n",
      "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "          heads.\n",
      "    \n",
      "    Example of single-label classification:\n",
      "    \n",
      "    ```python\n",
      "    >>> import torch\n",
      "    >>> from transformers import AutoTokenizer, BertForSequenceClassification\n",
      "    \n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
      "    >>> model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
      "    \n",
      "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "    \n",
      "    >>> with torch.no_grad():\n",
      "    ...     logits = model(**inputs).logits\n",
      "    \n",
      "    >>> predicted_class_id = logits.argmax().item()\n",
      "    >>> model.config.id2label[predicted_class_id]\n",
      "    'LABEL_1'\n",
      "    \n",
      "    >>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
      "    >>> num_labels = len(model.config.id2label)\n",
      "    >>> model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", num_labels=num_labels)\n",
      "    \n",
      "    >>> labels = torch.tensor([1])\n",
      "    >>> loss = model(**inputs, labels=labels).loss\n",
      "    >>> round(loss.item(), 2)\n",
      "    0.01\n",
      "    ```\n",
      "    \n",
      "    Example of multi-label classification:\n",
      "    \n",
      "    ```python\n",
      "    >>> import torch\n",
      "    >>> from transformers import AutoTokenizer, BertForSequenceClassification\n",
      "    \n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
      "    >>> model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", problem_type=\"multi_label_classification\")\n",
      "    \n",
      "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "    \n",
      "    >>> with torch.no_grad():\n",
      "    ...     logits = model(**inputs).logits\n",
      "    \n",
      "    >>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
      "    \n",
      "    >>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
      "    >>> num_labels = len(model.config.id2label)\n",
      "    >>> model = BertForSequenceClassification.from_pretrained(\n",
      "    ...     \"textattack/bert-base-uncased-yelp-polarity\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
      "    ... )\n",
      "    \n",
      "    >>> labels = torch.sum(\n",
      "    ...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
      "    ... ).to(torch.float)\n",
      "    >>> loss = model(**inputs, labels=labels).loss\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pt_model.__call__)\n",
    "help(pt_model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n",
      "        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "print(pt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "my_config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", n_heads=12)\n",
    "print(my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): DistilBertSdpaAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "my_model = AutoModel.from_config(my_config)\n",
    "print(my_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
